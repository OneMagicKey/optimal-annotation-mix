{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FiFjW07PeeJm",
    "outputId": "dd3a7efb-1765-4432-e5be-7c172fd56cc4"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/OneMagicKey/optimal-annotation-mix.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PascalVOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install all the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd optimal-annotation-mix/yolov5\n",
    "%pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F = 800, W = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Zv-Kb56iPJG",
    "outputId": "30a5a489-2c5c-405a-d033-28d2248127fc"
   },
   "outputs": [],
   "source": [
    "!python segment/train.py --img 512 --batch-size 16 --f-size 2 --epochs 75 --data voc_seg/voc-seg-fw-800-3000.yaml --hyp hyp.scratch-low.yaml --weights yolov5x-cls.pt --cfg yolov5x-seg.yaml --cache --no-overlap --patience 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "en_3vqCOen_T"
   },
   "outputs": [],
   "source": [
    "!cd .. && rm -r datasets/VOC/images datasets/VOC/labels && cd yolov5  # remove previous training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F = 800, W = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python segment/train.py --img 512 --batch-size 16 --epochs 400 --data voc_seg/voc-seg-fw-800.yaml --hyp hyp.scratch-low.yaml --weights yolov5x-cls.pt --cfg yolov5x-seg.yaml --cache --no-overlap --patience 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "en_3vqCOen_T"
   },
   "outputs": [],
   "source": [
    "!cd .. && rm -r datasets/VOC/images datasets/VOC/labels && cd yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pS7OLKoClwGp",
    "outputId": "477af933-53de-4b58-fb23-38493b3d516e"
   },
   "outputs": [],
   "source": [
    "!python segment/train.py --img 512 --batch-size 16 --f-size 1 --epochs 500 --data voc_seg/voc-seg-few-shot.yaml --hyp hyp.scratch-low.yaml --weights yolov5x-cls.pt --cfg yolov5x-seg.yaml --cache --no-overlap --patience 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fv0OULaghurA"
   },
   "outputs": [],
   "source": [
    "!cd .. && rm -r datasets/VOC/images datasets/VOC/labels && cd yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5mHtxSphniw"
   },
   "source": [
    "### Cityscapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install all the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6L9IP-Nhr2O",
    "outputId": "93c1c8ae-fc10-4af2-e763-09d9eb0caa6b"
   },
   "outputs": [],
   "source": [
    "%cd optimal-annotation-mix/yolov5\n",
    "%pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract _Cityscapes.zip_ to _datasets/Cityscapes_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IiyokB8jCLB",
    "outputId": "c75dd5e2-e445-40fc-ef73-3800cd86ef63"
   },
   "outputs": [],
   "source": [
    "# yolov5 is the current dir\n",
    "path_to_cityscapes = 'path/to/Cityscapes.zip'\n",
    "!cd .. &&  mkdir -p 'datasets/Cityscapes' && unzip -nq $path_to_cityscapes -d 'datasets/' && cd yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F = 1475, W = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7njkBXhh8ZM",
    "outputId": "aa564a03-f61e-4648-9be8-fd09795a5c5e"
   },
   "outputs": [],
   "source": [
    "!python segment/train.py --img 1024 --batch 8 --f-size 4 --epochs 272 --data cityscapes_seg/Cityscapes-seg-fw-1475-1000.yaml --hyp hyp.scratch-low.yaml --weights yolov5x-cls.pt --cfg yolov5x-seg.yaml --cache --no-overlap --patience 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jI8gzYOj_GK",
    "outputId": "c82723ec-4d6a-40f3-e21c-f2112b65d6a5"
   },
   "outputs": [],
   "source": [
    "!cd .. && rm -r datasets/Cityscapes/data/images datasets/Cityscapes/data/labels && cd yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F = 800, W = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python segment/train.py --img 1024 --batch 8 --epochs 500 --data cityscapes_seg/Cityscapes-seg-fw-800-0.yaml --hyp hyp.scratch-low.yaml --weights yolov5x-cls.pt --cfg yolov5x-seg.yaml --cache --no-overlap --patience 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && rm -r datasets/Cityscapes/data/images datasets/Cityscapes/data/labels && cd yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlqVTHajb91S"
   },
   "source": [
    "## DETR training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install all the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUH9bwsPcCnP",
    "outputId": "cb96d984-545b-4bed-8531-1a1308baad41"
   },
   "outputs": [],
   "source": [
    "%cd optimal-annotation-mix/detr\n",
    "%pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbFp0gDYiJuO"
   },
   "source": [
    "### Prepare pascal voc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L3Cxww0ty--q"
   },
   "outputs": [],
   "source": [
    "# In Colab, open /usr/local/lib/python3.10/dist-packages/pycocotools/cocoeval.py  \n",
    "# and replace iouThr=.75 with iouThr=.70 in line 462 to get mAP70 instead of mAP75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download PascalVOC dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9rzejAtcdE7",
    "outputId": "65151c02-7e17-4836-eb77-ec0e9699f522"
   },
   "outputs": [],
   "source": [
    "!mkdir -p 'dataset/annotations/segmentation' 'dataset/annotations/detection'\n",
    "!mkdir -p 'dataset/train' 'dataset/val'\n",
    "!bash VOC2012.sh 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-9pu999cjgx"
   },
   "outputs": [],
   "source": [
    "from shutil import move, copy\n",
    "\n",
    "\n",
    "path_to_VOC = 'dataset/VOCdevkit/VOC2012'\n",
    "\n",
    "ann_dir = f'{path_to_VOC}/Annotations'\n",
    "path_seg_val = f'{path_to_VOC}/ImageSets/Segmentation/val.txt'\n",
    "path_mask = f'{path_to_VOC}/SegmentationObject'\n",
    "\n",
    "def copy_data(ids_path, path_from, split):\n",
    "    if isinstance(ids_path, str):\n",
    "        with open(ids_path, 'r') as f:\n",
    "            ids = f.read().split()\n",
    "    else:\n",
    "        ids = ids_path\n",
    "\n",
    "    for i in ids:\n",
    "        img_path_from = f'{path_from}/{i}.jpg'\n",
    "        img_path_to = f'dataset/{split}/{i}.jpg'\n",
    "        copy(img_path_from, img_path_to)\n",
    "\n",
    "\n",
    "# def copy_detection():\n",
    "#     for split in ['train', 'val']:\n",
    "#         copy_data(f'{path_to_VOC}/ImageSets/Main/{split}.txt', f'{path_to_VOC}/JPEGImages', split)\n",
    "\n",
    "def copy_detection_trainval():\n",
    "    with open(f'{path_to_VOC}/ImageSets/Main/trainval.txt', 'r') as f:\n",
    "        trainval_ids = f.read().split()\n",
    "\n",
    "    with open(f'{path_to_VOC}/ImageSets/Segmentation/val.txt', 'r') as f:\n",
    "        val_seg_ids = f.read().split()\n",
    "        val_seg_ids = set(val_seg_ids)\n",
    "\n",
    "    trainval_det = sorted([i for i in trainval_ids if i not in val_seg_ids])\n",
    "    copy_data(trainval_det, f'{path_to_VOC}/JPEGImages', 'train')\n",
    "\n",
    "    split = 'val'\n",
    "    copy_data(f'{path_to_VOC}/ImageSets/Segmentation/{split}.txt', f'{path_to_VOC}/JPEGImages', split)\n",
    "\n",
    "def copy_segmentation():\n",
    "    for split in ['train', 'val']:\n",
    "        copy_data(f'{path_to_VOC}/ImageSets/Segmentation/{split}.txt', f'{path_to_VOC}/JPEGImages', split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create validation annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHyyMczbcl8Z",
    "outputId": "b59d6353-7fa8-4482-b554-5267736b977c"
   },
   "outputs": [],
   "source": [
    "copy_detection_trainval()\n",
    "copy_segmentation()\n",
    "\n",
    "# create val.json for detection\n",
    "!python voc2coco.py --ann_dir $ann_dir \\\n",
    "         --ann_ids $path_seg_val \\\n",
    "         --labels voc_labels.txt \\\n",
    "         --ext 'xml' \\\n",
    "         --extract_num_from_imgid \\\n",
    "         --output dataset/annotations/detection/val.json\n",
    "\n",
    "# create val.json for segmentation\n",
    "!python voc2coco.py --ann_dir $ann_dir \\\n",
    "         --ann_ids $path_seg_val \\\n",
    "         --labels voc_labels.txt \\\n",
    "         --ext 'xml' \\\n",
    "         --extract_num_from_imgid \\\n",
    "         --masks_path $path_mask \\\n",
    "         --output dataset/annotations/segmentation/val.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INJ6VuI8tquL"
   },
   "source": [
    "Given that DETR is trained in a two-stage manner, the initial stage is the training of the detection model, followed by the fine-tuning of the mask head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the YOLO, we created annotation files and placed them in _detr/data_. These files are named as follows: *train\\_{task}\\_{**F** size}\\_{**W** size}.txt*. To train DETR with **F** = 400, **W** = 10331 use _train_det_400_10331.txt_ in the detection step and _train_seg_400.txt_ in the segmentation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2njvsV5mcwMN"
   },
   "source": [
    "### Detection model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training annotation for the **detecton** task.\n",
    "\n",
    "Use 10331 detection + 400 segmentation images for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9v_KT1jikig",
    "outputId": "5ee9de63-1383-4b4b-af2f-c45bb678ba24"
   },
   "outputs": [],
   "source": [
    "path_train_det = f'data/train_det_400_10331.txt'\n",
    "\n",
    "!python voc2coco.py --ann_dir $ann_dir \\\n",
    "         --ann_ids $path_train_det \\\n",
    "         --labels voc_labels.txt \\\n",
    "         --ext 'xml' \\\n",
    "         --extract_num_from_imgid \\\n",
    "         --output dataset/annotations/detection/train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJ1ruqeWc6Sp"
   },
   "outputs": [],
   "source": [
    "dataset_file = \"voc\"\n",
    "dataDir='dataset'\n",
    "outDirDet = 'outputs/detection/train_det_400_10331'\n",
    "\n",
    "!python main.py \\\n",
    "  --dataset_file $dataset_file \\\n",
    "  --batch_size 10 \\\n",
    "  --coco_path $dataDir \\\n",
    "  --output_dir $outDirDet \\\n",
    "  --epochs 300 \\\n",
    "  --lr_drop 250 \\\n",
    "  --eos_coef 0.03 \\\n",
    "  --num_queries 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a1oeVlSzOaN"
   },
   "source": [
    "### Segmentation model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training annotation for the **segmentation** task.\n",
    "\n",
    "Use the same 400 segmentation images for training the mask head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ew6QGzy8jXNe",
    "outputId": "f81bcb54-5665-41f9-e72f-b7c8a682b0f1"
   },
   "outputs": [],
   "source": [
    "path_train_seg = f'data/train_seg_400.txt'\n",
    "\n",
    "!python voc2coco.py --ann_dir $ann_dir \\\n",
    "         --ann_ids $path_train_seg \\\n",
    "         --labels voc_labels.txt \\\n",
    "         --ext 'xml' \\\n",
    "         --extract_num_from_imgid \\\n",
    "         --output dataset/annotations/segmentation/train.json \\\n",
    "         --masks_path $path_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8hQaERBzW3R",
    "outputId": "681dda2e-ceb4-4edf-8f46-25d7d5e024c6"
   },
   "outputs": [],
   "source": [
    "dataset_file = \"voc\"\n",
    "dataDir='dataset'\n",
    "outDirSeg = 'outputs/segmentation/train_seg_400_10331'\n",
    "\n",
    "# Path to the detection model checkpoint from the previous step\n",
    "frozen_weights = f'{outDirDet}/checkpoint.pth'\n",
    "\n",
    "!python main.py \\\n",
    "  --dataset_file $dataset_file \\\n",
    "  --batch_size 3 \\\n",
    "  --frozen_weights $frozen_weights \\\n",
    "  --coco_path $dataDir \\\n",
    "  --output_dir $outDirSeg \\\n",
    "  --epochs 150 \\\n",
    "  --lr_drop 120 \\\n",
    "  --eos_coef 0.03 \\\n",
    "  --num_queries 100 \\\n",
    "  --masks"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "D5mHtxSphniw"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
